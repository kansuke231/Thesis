\documentclass{article}
\usepackage{enumitem}
\usepackage{fixltx2e}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage[tbtags]{amsmath}
\usepackage{textcomp}
\usepackage{subcaption}
\usepackage{float}
\usepackage[top=25truemm,bottom=30truemm,left=25truemm,right=25truemm]{geometry}



\setlength\intextsep{5pt}
\setlength\textfloatsep{5pt}


\def\vector#1{\mbox{\boldmath $#1$}}
\begin{document}

\title{Inter-domain Relationships of Complex Networks}
\author{Kansuke Ikehara}
\maketitle

\begin{abstract}
The structure of complex networks has been of interest in many scientific and engineering disciplines over decades. A number of scientists have reported findings that indicated the characteristics of network structure that are claimed to be universal, such as \textit{small-world} and \textit{small-world} networks. Domain specific characteristics, for example specific profiles of network motifs and high-triangle density in social networks, have also been reported previously. However, there is no comprehensive study of inter-domain relationships of complex networks based on the network structure. In this paper, we study over 1200 of real-world networks of diverse domains ranging from ecological food-web to online social network along with networks generated from four popular network models. Our study is data-driven, utilizing machine learning techniques such as random forest and confusion matrix and we show the relationships among network domains in terms of network structure. Our result indicates that there are some pairs of network domains (categories) that are inherently hard to distinguish based purely on network structure. We have found that these domains tend to have similar underlying mechanisms which generate a network or drive the growth of a network or processes which happen on a network itself.
  
\end{abstract}
\tableofcontents


\section{Introduction}
	\subsection{Complex Networks}
	Almost every scientific and engineering discipline deals with data that come from experimental observations or simulations running on a computer. Traditionally, data have been numerals, which may represent temperature, velocity, or voltage, and methodologies that analyze these numerals have been established for over hundreds of years. 
	
	\textit{Graph} or \textit{network}, which is essentially structured data as opposed to simple numbers, has been used as a new way to approach real-world problems in the last decades. Social science, for instance, has witnessed the power of network analysis with the recent emergence of online social network services such as Facebook, Twitter, LinkedIn, etc.  Social scientists have been solving previously intractable questions in the field with the large amount of  data and advanced graph algorithms with high-performance computers. Biological sciences use networks as a tool to dissect biological, chemical and even ecological processes in order to gain insight into how neurons in our brain interact with each other, how nutrients get broken down into pieces and get re-assembled in another molecular form, and how animals eat or be eaten by other animals. Engineering systems such as the Internet, power grids, water distribution networks, transportation networks, etc. have also been investigated with using network analysis tools for constructing efficient and robust systems. 
	
	
	\subsection{Structural Universality of Networks}
	
	One of the main research themes in Network Science, an interdisciplinary field which studies properties of networks ranging from biological and social networks to engineering system networks, is to find hidden universal patterns in the networks of various categories. 
	Watts and Strogatz proposed a random network model that, depending on the parameter setting, produces a network having properties as follows: 1. high density of triangles, implying that if three nodes are connected, it is likely that those three nodes actually compose a triangle; 2. low average distance between a pair of nodes, which indicates that from any node it is just a few steps, in average, to reach any other node in the network \cite{watts1998cds}. Networks having these attributes are often called "small-world" networks and many kinds of networks have been reported as having the small world property \cite{Network_Small-World-Ness}.

	
	
	 Another property that is claimed to be universal by some researchers, yet that is still controversial, is \textit{power-law} degree distribution which is sometimes referred to as \textit{scale-free}. In a scale-free network, one observes a number of nodes with few connections while there are few nodes with many connections to others. This very skewed distribution of degree, connections attached to a node, is claimed to be found among diverse sets of networks \cite{Barabasi99emergenceScaling}. However, claims that a network follows the power-law degree distribution often lacks a valid statistical treatment in order to show it is indeed following the distribution \cite{Clauset:PowerLaw}


	\subsection{Structural Diversity of Networks}
	Contrary to prevalent small-world and scale-free properties, each category of networks exhibits unique structural profiles of its own. Social networks have been investigated by many researchers and found out that they exhibit unique structural characteristics: high density of triangle (clustering coefficient) and positive degree assortativity \cite{WhySocialNetworks, Mislove:2007:OnlineSocial}. High triangle density indicates the high probability of one's friends being friends each other and the positive  degree assortativity shows a tendency that high-degree nodes (nodes having many connections) connect to other high-degree nodes while low-degree nodes connect to other low-degree nodes.
	
	Roth \textit{et al.} identified a unique structure of subway networks, that is called core-branch structure in the literature \cite{Train}. Many real world subway networks have a ring-shape connections of stations and dense connections therein, referred to as the "core" of the network. In the core, stations are relatively densely connected to each other, enabling residents of a city to move around quickly. From the ring of the core, branches radiate outward connecting stations far from the center of the city. 
	
	Milo \textit{et al.} introduced a concept called \textit{network motifs} that are essentially patterns of frequent sub-graph (motif) in a particular network compared to its randomized network\cite{Milo_motif}. They have shown each category of network, such as gene regulation (transcription), food webs, electronic circuits, etc., has a distinct pattern of network motifs. Furthermore, Milo \textit{et al.} revealed the existence of superfamilies of networks that  are groups of network categories having the highly convergent motif profiles \cite{Milo_SuperFamily}. Onnela \textit{et al.} constructed a taxonomy of 746 networks  of diverse categories using "Mesoscopic Response Functions" (MRFS) that essentially describes the changes of specific values related to the community structure of a network with respect to a parameter $\xi \in [0,1]$\cite{Onnela_Taxonomy}. Each network has its own MRFS and they calculated distance between networks which is defined as the area of difference between two networks' MRFS. They have successfully identified clusters of similar networks in a dendrogram constructed based on MRFS distance.
	
	\subsection{Searching for The Underlying Principles}
Network is merely a simplified mathematical object and yet, one could gain some insight into a phenomenon and/or process of a studied field just from network structure. 
If there is a diverse set of networks of various kinds and scales, what can we say about inter-domain relationships between network categories? The question is threefold: 
\begin{enumerate}
	\item \textit{What aspects of network structure make a specific category of network different from others?}
	\item \textit{Are there any sets of network categories that are inherently indistinguishable from each other based on network structure?} 
	\item \textit{If two networks of different categories are indistinguishable by network structure, are their mechanisms of underlying processes the same? And vice versa.}
\end{enumerate}

The first part of the question is in some sense an extension to studying social networks' structural uniqueness. For example, what kinds of network structure is unique in, say metabolic networks? As far as we know, few previous studies have done extensive investigation in finding unique characteristics of specific kinds of networks compared to other kinds except social network.

The second part of the question asks if there is any structural similarity between different kinds of networks. We, however, use the word "indistinguishable" in stead of "similar" since we try to observe the commonality from a confusion matrix of a classifier, where a misclassified instance is considered to be indistinguishable from a class it's classified as because it is so similar to other instances of the wrong class the algorithm fails to label the instance correctly.


The last part of the question is essentially all about elucidating the meta-structure in domains or class labels. What if two very distinct categories (labels) of networks, say biological and technological ones, exhibit very similar network structure and a classifier misclassifies them many times? Is this because their underlying network generative processes, or their processes on networks themselves are the same? Answering this question helps us understand a driving mechanism for network structure.


The contribution of answering those questions comes in two ways: 1 (scientific point). it gives us a general conceptual framework upon which networks are studied across domains. Previous studies have only looked at a single category or multiple categories as one category, ignoring the relationships between categories. By studying networks across domains, one could find general theories of networks or test a hypothesis across domains in a more plausible way.  For instance, one could test validity of a network model across all of domains and find out which domains the network model can explain well; 2 (practical point). it gives us the knowledge base upon which various network-related algorithms could be constructed and tuned properly.  Many practical graph algorithms take no assumption in domain-specific network structure. It may be possible, however, to construct a new algorithm which runs faster and more efficiently on a specific kind of networks by taking into account of such domain-specific knowledge. For example, if there exists any unique network structural property of recommendation networks,  it may be applied to construct or fine tune a recommendation engine that utilizes knowledge of unique network structure. As the size of networks we obtain has grown in an unprecedented scale, domain-specific knowledge in network structure will be a key to analyze such large networks in a faster and more efficient way.

\subsection{Project Description}

In this thesis, we study 1200 (the number may change later) of various kinds of real- world networks, ranging from ecological food-web to online social network to digital circuit along with synthetic networks generated from four different models. We extract network statistics as features of a network, construct a high-dimensional feature space, where each axis corresponds to one of the network features, and map each network onto the feature space. We then train a machine learning algorithm called \textit{random forest} with the training set of network data and let it classify the test networks based on their features. As the category distribution is skewed, meaning that some categories of networks have larger number of instances than other minority categories, we try several sampling strategies and show the effect of each methodology. We construct confusion matrices based upon classification results and proceed to analyze the misclassification with which we can answer research questions explained in the previous section. We then conclude with discussion based on several hypotheses and classification results.



\section{What Is A Network?}
	\subsection{Formal Definition}
	Formally, a network or graph is a mathematical object consisting of a set of edges (arcs) and nodes (vertices), which can be written as $G = (E,V)$. In many cases, an \textit{adjacency matrix} is used as a representation of a network, where each element of the matrix $A_{ij}$ takes a binary value, $1$ for presence of an edge between nodes $i$ and $j$, and $0$ if there is no edge between these nodes in an unweighted network, or real value if edges of the network are weighted. If the matrix is symmetric, namely $A_{ij} = A_{ji},  \forall i,j \in V$, the matrix represents an \textit{undirected} network in which edges do not have directionality at all. If the matrix is asymmetric, on the other hand, it's representing a \textit{directed} network, where edges have directionality. If the diagonal elements of a matrix $A$ are non-zero, the network has \textit{self-loops} which indicates that there are edges originating from and pointing to the exact same node. In many studies such self-loops are simply ignored for the sake of simplicity and out study is no exception as well.
	
	\subsection{Structural Features of A Network}
	There are many ways to characterize a network in a quantitative manner that use the network's structural features \cite{Newman:NetworksIntro,NetworkCharacterizationSurvey}. Some network features, however, are implicitly correlated with the size of networks, which itself is a very strong feature: the number of nodes in web graphs is usually a magnitude of $10^6$ or more whereas ecological food webs contain usually less than 100 nodes. The feature set we use in this study is scale-invariant, meaning the size of network is supposed not to effect the value of a feature. This set of features allows us to compare networks without the notion of network size. In the following sections we describe important structural features that are relevant in this study.
		
	 
	\subsubsection{Degree}
	The \textit{degree} of a node in a network is essentially the number of edges attached to the node. For a node $i$ in an unweighted network, the degree $k_i$ of the node can be written mathematically as:
	
	\begin{equation}
 	 k_i = \sum_{j = 1}^n A_{ij}.
	\end{equation}
	
	In a directed network, there are two kinds of degree, namely \textit{in-degree} and \textit{out-degree}, each of which represents the degree of a node $i$ for incoming edges and outgoing edges respectively:
	
	\begin{equation}
 	 k_i^{in} = \sum_{j = 1}^n A_{ij},
	\end{equation}
	
	\begin{equation}
 	 k_i^{out} = \sum_{i = 1}^n A_{ij}.
	\end{equation}

	
	\subsubsection{Clustering Coefficient}
	Clustering coefficient, which describes a degree of \textit{transitivity} in a network, is one of the most widely used metrics in network analysis especially in the context of social network. Transitivity in the context of networks means that if nodes $a$ and $b$ are connected as well as nodes $b$ and $c$, then nodes $a$ and $c$ are connected. Mathematically, the definition of clustering coefficient is given by the following equation:
	\begin{equation}
	C = \frac{\text{number of closed paths of length two}}{\text{number of paths of length two}}.
	\end{equation}
	
	\subsubsection{Degree Assortativity}
	\textit{Assortativity} in a network indicates a tendency for nodes to be connected to other nodes with similar node attribute. In a social network, for instance, node attributes which could be the basis for assortativity include language, age, income, alma mater, etc. On the other hand, \textit{disassortativity} exhibits nodes' tendency to be connected to other nodes having different node attribute in the network. An example network of disassortatvity would be a social network of sexual-relationship among people. 
	\textit{Degree assortativity} is a form of assortativity in which nodes with similar degree value tend to be connected together. Therefore in a network exhibiting high degree assortativity, there is a core of highly connected nodes with the high degree and periphery of nodes sparsely connected to other low-degree nodes.
	
	Here, we first derive a general \textit{assortativity coefficient} of an undirected, unweighted network based on scalar node attribute, excluding enumerative ones such as language, alma mater, etc., then we easily find \textit{degree assortativity coefficient}.
	
	First we calculate the covariance of $x_i$ and $x_j$ for the vertices at the ends of all edges, where $x_i$,$x_j$ are nodes $i$ and $j$'s attributes.
	The mean $\mu$ of the value of $x_i$ at the end of one edge is defined as:
	\begin{equation}
	\mu = \frac{ \sum_{ij}A_{ij}x_i }{\sum_{ij}A_{ij}} = \frac{\sum_i k_i x_i}{\sum_i k_i} = \frac{1}{2m}\sum_i k_i x_i,
	\end{equation}
where $k_i$ is a degree of node $i$.

Then, the covariance of of $x_i$ and $x_j$ over all edges is derived as:
	\begin{equation}
	\begin{split}
	\text{cov}(x_i, x_j)  &= \frac
					{ \sum_{ij}A_{ij}(x_i - \mu)(x_j - \mu) }
					{\sum_{ij}A_{ij} } \\
					&= \frac{1}{2m}\sum_{ij}A_{ij} (x_i x_j - \mu x_i - \mu x_j +\mu^2) \\
					&= \frac{1}{2m}\sum_{ij}A_{ij}x_i x_j - \mu^2 \\
					&= \frac{1}{2m}\sum_{ij}A_{ij}x_i x_j - \frac{1}{(2m)^2}\sum_{ij}k_i k_j x_i x_j \\
					&= \frac{1}{2m}\sum_{ij}(A_{ij} - \frac{k_i k_j}{2m})x_i x_j.
	\end{split}
	\end{equation}
This covariance will be positive if both $x_i$ and $x_j$ have, in average, similar values, and will be negative if both $x_i$ and $x_j$ tend to change in a different direction. We normalized cov($x_i$,$x_j$) by another quantity which represents a perfect assortativity, so that it takes a value $r \in [-1 ,1]$. The perfect matching happens if $x_i = x_j$ for all edges, and cov($x_i$,$x_i$) becomes:
	\begin{equation}
	\begin{split}
	 \text{cov}(x_i, x_i) &=  \frac{1}{2m}\sum_{ij}(A_{ij} x_i^2 - \frac{k_i k_j}{2m} x_i x_j) \\
	 		             &=  \frac{1}{2m}\sum_{ij}(k_i \delta_{ij} - \frac{k_i k_j}{2m}) x_i x_j,
	\end{split}
	\end{equation}
	where $\delta_{ij}$ is Kronecker delta.
Thus, the normalized covariance becomes as follows:
	\begin{equation}
	\begin{split}
	 r = \frac{ \text{cov}(x_i, x_j)}{ \text{cov}(x_i, x_i)} = 
	 \cfrac{\sum_{ij}(A_{ij} - \cfrac{k_i k_j}{2m})x_i x_j}{\sum_{ij}(k_i \delta_{ij} - \cfrac{k_i k_j}{2m}) x_i x_j}.
	\end{split}
	\end{equation}

Degree assortativity coefficient can easily be obtained by substituting $x_i$ and $x_j$ with degrees of respective vertices, thus:
	\begin{equation}
	 r =  \cfrac{\sum_{ij}(A_{ij} - \cfrac{k_i k_j}{2m})x_i x_j}{\sum_{ij}(k_i \delta_{ij} - \cfrac{k_i k_j}{2m}) k_i k_j}.
	\end{equation}

	\subsubsection{Network Motifs} 
The idea of \textit{network motifs} was first introduced by Milo \textit{et al.} \cite{Milo_motif}. Network motif is a sub-graph of a network that appears statistically significant than in its randomized counterparts, usually random networks having the same degree distribution. There are a number of studies using network motifs of directed networks, where edges have directions, especially in biological sciences \cite{Alon2007, MotifsInBrain}. We, however, only use "connected" network motifs of undirected networks, that produce fewer variations in motif kinds, but can be applied to any network regardless of edge directionality which is crucial for our study. Fig. \ref{motifs} shows the complete list of $k=4$ undirected connected motifs used in this study.

\begin{figure}[ht]
	\begin{center}
		\vspace{0.5cm}
		\includegraphics[clip,width=12cm,height = 3cm]{figs/motifs.png}
		\vspace{0.5cm}
		\caption{Network motifs of $k =4$, where $k$ is the number of node in a connected sub-graph.}
		\label{motifs}
	\end{center}
\end{figure}

In order to quantify network motifs, we first count the occurrence of each sub-graph in the original network, then repeat the same process on the randomized networks that have the exact same degree distribution. Such random networks are often referred to as \textit{configuration model} and are widely used as a null model for calculating various statistics related to networks. After counting occurrences of sub-graphs in both original and multiple random networks, we proceed to calculate z-score for each sub-graph $i$ as follows:
	\begin{equation}
	Z_i = \cfrac{N_i^{\text{original}} - \langle N_i^{\text{random}} \rangle }{\sigma_i^{\text{random}}},
	\end{equation}

where $N_i^{\text{original}}$ is the number of occurrence of a sub-graph $i$ in the original network and $ \langle N_i^{\text{random}} \rangle$ and $\sigma_i^{\text{random}}$ are the average and the standard deviation of the number of occurrence of a sub-graph $i$ in an ensemble of random networks. It is usually convenient to normalize this z-score as some networks exhibits very large values due to the size of the networks. Such normalized z-score is called \textit{significance profile} and is defined as follows:
	\begin{equation}
	SP_i = \cfrac{Z_i}{\sum_j Z_j^2}.
	\end{equation}
	
\section{Data Sets}
	
	Our network data set has been accumulated with a lot of effort over years and links to most of the data are available at \url{https://icon.colorado.edu}.
	Since data format of real world networks is not standardized, we proceeded to convert all the data into a single format called \textit{Graph Modeling Language} or simply GML \cite{GML}. The format allows us to flexibly specify arbitrary node and edge attributes. We, however, do not use any node and edge attribute, including edge weight, as our study only focuses on simple network structure.
	
	We have also added some synthesized network data which are generated from specific models as follows: 1. Erd\H{o}s-R\'enyi random network (ER Network) \cite{ER_Network}, Watts-Strogatz model (Small World) \cite{watts1998cds}, Barab\'asi-Albert model (Scale Free) \cite{Barabasi99emergenceScaling} and the forest fire model (Forest Fire Network) \cite{ForestFire}
	
	The distributions of network domains and sub-domains, as shown in figs.\ref{domain_ratio} and \ref{sub_dist}, are very skewed since instances of some network categories are hard to obtain due to their inherent difficulty of collecting data or legal concerns, or hard to analyze due to their network size. This skewed categorical distribution leads us to explore several sampling strategies, which are explained in the following chapter.
	
	
	\begin{figure}[ht]
	\begin{center}
		\vspace{0.5cm}
		\includegraphics[clip,width=12cm,height = 12cm]{figs/category_ratio.png}
		\vspace{0.5cm}
		\caption{The categorical ratio of network domains.}
		\label{domain_ratio}
	\end{center}
	\end{figure}
	
	\begin{figure}[ht]
	\begin{center}
		\vspace{0.5cm}
		\includegraphics[clip,width=19cm,height = 12cm]{figs/subdomain_dist.png}
		\vspace{0.5cm}
		\caption{Count distribution of network sub-domains. Sub-domains of the same network domain are grouped together having the same color in the figure. Color code from top: Biological, Economic, Informational, Social, Synthesized, Technological and Transportation.}
		\label{sub_dist}
	\end{center}
	\end{figure}

	

\section{Methodology}

	\subsection{Sampling Strategies}
Most of machine learning algorithms perform well on evenly populated instances of multiple classes. However, once this class balance no longer persists, the algorithms perform poorly on minority classes . This problem, called \textit{class imbalance}, essentially causes any machine learning algorithm being naive to the data set to focus exclusively on the majority class, ignoring any minority classes. One of the most widely used approaches for mitigating the problem is sampling the data set of interest so that the distribution of classes becomes balanced. Although there are many proposed sampling strategies as of now \cite{SurveySampling}, we primarily use three sampling strategies in this study: random over-sampling, random under-sampling and SMOTE \cite{SMOTE}.


		\subsubsection{Random Over/Under Sampling}
		Random sampling method is one of the simplest strategies for mitigating the class imbalance problem. \textit{Random Oversampling} essentially over-samples any minority classes to an extent that the number of instances in each class becomes even. However, this could potentially make a classifier overfit to points that are over-sampled, therefore lead to poor generalization. \textit{Random Undersampling}, on the other hand, under-samples the majority class, essentially throwing out some data in order to make the "cloud" of data points sparser. This "throwing out instances" implies an obvious drawback of this sampling method: It discards potentially important instances that compose the backbone of majority classes, implying the true shape of majority class is no longer retained.
		
		\subsubsection{SMOTE}
		\textit{Synthetic Minority Over-sampling Technique}, widely known as SMOTE \cite{SMOTE}, is an alternative sampling method that synthesizes data points in the training set for a classifier. The high level description of the sampling technique can be explained as follows:
		
		\begin{enumerate}
			\item Ignore the majority class. If classification is multi-class, ignore all of the classes except the class on which one is going to sample.
			\item For every minority class instance, choose its $k$ nearest neighbors. This $k$ is a parameter of the sampling algorithm.
			\item On every line segment between a chosen data point and its  $k$ nearest neighbors, create new instances and place them randomly. See Figure \ref{smote} for this process.
		\end{enumerate}
		
		\begin{figure}[ht]
		\begin{center}
		\vspace{0.5cm}
		\includegraphics[clip,width=12cm,height = 3cm]{figs/SMOTE.png}
		\vspace{0.5cm}
		\caption{Synthesizing phase of SMOTE. Here the number of nearest neighbors $k$ is 3.}
		\label{smote}
		\end{center}
		\end{figure}

The core concept of SMOTE is filling out the cloud of minority class instances by interpolating existing data points so it closely resembles a \textit{convex set}. This idea, making a convex set of minority instances by interpolation, assumes that the underling data distribution itself is convex. In a high dimensional space, it is often the case that the distribution of data forms a quite intricate manifold from which the data we observe is generated. So interpolating data points of the complex manifold yields a convex set, that is radically different from the underlying concept that a classifier tries to learn.

	\subsection{Classification}
	In order to find similarities and differences of data of different classes, one needs to develop a notion of "distance" between the classes. In this study we derive such notion of distance from confusion matrices that are produced by random forest classifiers. Following sub-sections describe the details of decision tree which is an essential component of random forest, random forest classifier and confusion matrix.
	
		\subsubsection{Decision Tree}
		Decision tree is a model which describes the relationship between input variables and output class by recursively asking a question on a single input variable and splitting the data set into two based on the answer to the question until a data set has enough homogeneity of a class in it. In a high dimensional space, such spitting the data set corresponds to hyperplane in the space. The algorithm for learning decision tree splits the data set based on a criterion of values of an input variable such that the resulting data sets become less heterogeneous or less "impure" in terms of class labels. One of the widely used such criteria and the one we use in this study is \textit{Gini impurity}. The definition of Gini impurity for a data set with $J$ classes is the following:
	\begin{equation}
	I_G(f) = \sum_{i=1}^J f_i(1-f_i) = \sum_{i=1}^J (f_i-f_i^2) =  \sum_{i=1}^J f_i - \sum_{i=1}^J f_i^2 = 1- \sum_{i=1}^J f_i^2,
	\end{equation}
where $f_i$ is a probability that an item that belongs to class $i$ is chosen in the data set. Gini impurity becomes $0$ if all items in a set belong to the same class, meaning the set is "pure" and takes a value greater than $0$ if the set contains items of multiple classes. Each splitting essentially seeks the best possible value of an input variable such that the decrease of Gini impurity is the largest when the data set is split at the value (or hyperplane defined with it). Splitting continues until no further improvement can be made and the terminal of a tree are called leaves of the tree, each corresponding to one of the class labels in the data set.
		
	
		\subsubsection{Random Forest Classifier}
Random forest is a type of \textit{ensemble learning method} that combines a number of so called "weak" classifiers together \cite{RandomForest}. When it's given a data set to predict after training weak classifiers, it outputs a majority of all outputs from the weak classifiers and this aggregation of weak classifiers prevents random forest from overfitting to the training data. In random forest, such weak classifiers are decision tree which is explained in the previous section.

The learning phase of random forest classifier first involves random sub-sampling of the original data with replacement for $B$ times, each time the sub-sampled data set is fed to a decision tree. For each decision tree a set of randomly sampled input variables (features) is used for splitting and this randomly selecting features is called random subspace method or feature bagging. This prevents the classifier to focus too much on feature sets that are highly predictive in the training set. 

One of the advantageous byproducts of random forest is that one could rank input variables or features based on the importance in the classification. Each time a split is made on a node in a decision tree, the decease of Gini impurity can be attributed to selecting a feature as a splitting point.  Calculating the average decrease of Gini impurity for selecting a feature over all decision trees in random forest gives us the importance of the feature that is very consistent with the result of original method for calculating variable importance \cite{RandomForest,RandomForestOnline}. This ranking of feature importance is the crucial part of the analysis we we describe in the next chapter.


		\subsubsection{Confusion Matrix, Similarity and Distance}
	Confusion matrix depicts when and how frequently a classifier makes mistakes. The row labels of the matrix usually correspond to  true labels and column labels correspond to predicted labels. An element $c_{ij}$ in a confusion matrix represents the number of occurrences that a classifier predicted an instance of class $i$ as class $j$. So it is easy to notice that diagonal elements of a confusion matrix, namely $c_{ii}$ for $i = 1,...,n$ represents the correct predictions of a classifier. What we are interested in, however, lies in off-diagonal elements of a confusion matrix. This information that a classifier gets confused with classes $i$ and $j$ implies the similarity or distance between classes: if the points of two different classes in a high dimensional feature space are often misclassified as each other,  
the points of the two classes are in fact overlapped to some extent in the feature space, implying that these classes are so similar the distance between them is small.

Using a confusion matrix as either similarity or distance matrix involves following issues:
\begin{enumerate}
	\item  In a confusion matrix the higher a value of an element $c_{ij}$ the more similar the classes $i$ and $j$. This is fine for a similarity matrix. In a distance matrix, however, similar classes should have a small value for a corresponding element $d_{ij}$.
	\item In a confusion matrix classes with abundant data tend to have large counts for elements in the matrix due to the abundance of test data whereas classes with fewer data have fewer counts in the matrix.
	\item Usually the confusion matrix is not symmetrical, but a number of similarity/distance-related methods assume an input matrix has the symmetry.
\end{enumerate}

Therefore we proceed on the following operations in order to derive a similarity/distance matrix based on a confusion matrix:
\begin{enumerate}
	\item Normalize each row $i$ of the confusion matrix so that $\sum_{j=1}^J c_{ij} = 1$
	\item Symmetrize the resulting matrix from operation $1$ by setting each pair of symmetric elements as: $c_{ij},c_{ji} = \max (c_{ij},c_{ji})$
	\item For a distance matrix, convert each element of normalized symmetrical confusion matrix as follows: $c_{ij} := 1 - c_{ij}$, so that similar classes have smaller values and dissimilar classes have large values.
\end{enumerate}

We utilize both similarity and distance matrices in experiments which are described in the subsequent chapter.
		
\section{Analyses}
In this chapter, we present experimental settings for analyses, each of which tries to gain some insights for answering the questions we proposed in the introduction, and show the results of such analyses.
\subsection{Discriminative Feature Set}
The first question we have asked was: \textit{What aspects of network structure make a specific category of network different from others?} This question can be answered by looking at the statistics of feature importance from random forest classifiers. The experimental setting is the following: 

\begin{enumerate}[label={}]
	\item First, we select a set of six representative network sub-domains since not all of the sub-domains have enough number of samples to support our findings. These representative sub-domains are: protein interaction, ecological food web, metabolic, connectome,  online social and communication (autonomous system)
	\item Second, for each representative class we proceed to run binary classification 1000 times using random forest in which all of the class labels are grouped together except the target class label. In each run we split the data set into training and test sets with the ratio of $7:3$ while preserving the ratio of class distribution. In each run the score of accuracy and AUC (Area Under the ROC Curve) is calculated and the ranking of feature importance is recorded. We then average accuracy and AUC scores over 1000 runs and aggregate all of the recorded rankings of feature importance.
	\item Lastly we select the two most importance features from the aggregated ranking, plot the all of the data points in the two dimensional feature space.
\end{enumerate}

Fig.\ref{2d_figures} shows two dimensional plots for representative network sub-domains with axes being the selected important features.
One important observation here is the accuracy of a binary classification does not necessary describe the separability of the target class from other aggregated classes. All of the classifications score more than 98\% which seems quite remarkable. Nevertheless, this is due to the effect of class imbalance as we do not sub/over-sample in this analysis. However, the score of AUC well captures the separability of two classes. 

Fig.\ref{feature_importance_figures} shows the aggregated rankings of feature importance. In this figure, one can observe the general trend of informative feature set and the "strength" of those features in the ranking. For example, the motif $m4\_6$ of metabolic networks and the motif $m4\_1$ of online social networks are the most important features and their strength is quite dominant:  $m4\_6$ of metabolic networks ranks as the first 996 times out of 1000 runs and $m4\_1$ of online social networks ranks as the first 963 times  out of 1000 runs, respectively.

%From these figures and the score of AUC, one could observe the structural uniqueness and underlying mechanism of some of the representative networks. Ecological food webs exhibit a very high score for the motif $m4\_4$ and the structure of this motif implicitly tells us the underlying mechanism of food web: 

\begin{figure}[H]
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/protein/2d.png}
\caption{Protein interaction. average accuracy: 98.07\%; average AUC score: 0.713} \label{protein_2d}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/food_web/2d.png}
\caption{Ecological food web. average accuracy: 99.76\%; average AUC score: 0.946} \label{foodweb_2d}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/metabolic/2d.png}
\caption{Metabolic. average accuracy: 99.1\%; average AUC score: 0.936} \label{metabolic_2d}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/connectome/2d.png}
\caption{Connectome. average accuracy: 98.71\%; average AUC score: 0.682} \label{connectome_2d}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/online_social/2d.png}
\caption{Online social. average accuracy: 99.16\%; average AUC score: 0.939} \label{online_social_2d}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/communication/2d.png}
\caption{Communication. average accuracy: 99.68\%; average AUC score: 0.976} \label{communication_2d}
\end{subfigure}

\caption{2D plots for all representative network sub-domains. X axis corresponds to the most importance feature and y axis the second most important.} \label{2d_figures}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/protein/feature_importance.png}
\caption{Protein interaction.} \label{protein_feature}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/food_web/feature_importance.png}
\caption{Ecological food web.} \label{foodweb_feature}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/metabolic/feature_importance.png}
\caption{Metabolic. } \label{metabolic_feature}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/connectome/feature_importance.png}
\caption{Connectome.} \label{connectome_feature}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/online_social/feature_importance.png}
\caption{Online social.} \label{online_social_feature}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/one_by_many/communication/feature_importance.png}
\caption{Communication.} \label{communication_feature}
\end{subfigure}

\caption{Aggregated rankings of feature importance. The height of a color bar indicates a frequency of the corresponding specific feature being at the rank. The importance decreases along the x-axis} \label{feature_importance_figures}.
\end{figure}


\subsection{Similarities in Networks of Different Kinds}
Every network belongs to some sort of network domain or sub-domain that usually describes the properties and even structure of a network. As we have seen in the previous section, some networks of selected representative sub-domains exhibit structural uniqueness which makes them stand out among others in the feature space.

However we can also observe the overlaps between networks of representative sub-domains and networks of other sub-domains in the Fig. \ref{2d_figures}, which leads to a question we have asked:  \textit{Are there any sets of network categories that are inherently indistinguishable from each other based on network structure?} In this section we explore the structural similarities of different kinds of network domains and sub-domains using machine learning techniques such as random forest, confusion matrix and dendrogram of hierarchical clustering.  

\subsection{Experimental Settings}
We derive structural similarities/distance between network (sub-) domains from a confusion matrix that describes when a random forest classifier makes mistakes and when it does not. However, due to the nature of the classification algorithm and randomly splitting the data into training and testing sets, there involves some randomness in a confusion matrix every time one runs the analysis. Therefore, in order to alleviate a factor of randomness as much as possible, we run the analysis 1000 times and average the outcomes, namely confusion matrices and feature importance. 

In order to see the effect of class imbalance problem, we set four sampling strategies: no-sampling, namely run the analysis on the original data set; random over-sampling in which minority classes are over-sampled to an extent where all classes have the same number of instances as the largest class; random under-sampling in which majority classes are under-sampled to an extent where all classes have the same number of instances at the smallest class; SMOTE in which all minority classes have synthesized new instances so that the number of data points equals to the one of the largest class.
 
 
\subsection{Network Domains}

\begin{figure}[H]
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/similarity/Domain/None/confusion.png}
\caption{No sampling.} \label{no_confusion}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/similarity/Domain/RandomOver/confusion.png}
\caption{Random over-sampling.} \label{random_over_confusion}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/similarity/Domain/RandomUnder_26/confusion.png}
\caption{Random under-sampling.} \label{random_under_confusion}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/similarity/Domain/SMOTE/confusion.png}
\caption{SMOTE.} \label{somte_confusion}
\end{subfigure}
\
\caption{Confusion matrices for each of the sampling strategy.} \label{confusion}
\end{figure}


\clearpage

\bibliographystyle{ieeetr}
\bibliography{reference} 

\end{document}












 
 